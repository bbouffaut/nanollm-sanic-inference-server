#!/bin/bash

MAKE_CMD=$1

docker run --runtime nvidia --env NVIDIA_DRIVER_CAPABILITIES=compute,utility,graphics --shm-size=8g --volume /tmp/argus_socket:/tmp/argus_socket --volume /etc/enctune.conf:/etc/enctune.conf --volume /etc/nv_tegra_release:/etc/nv_tegra_release --volume /tmp/nv_jetson_model:/tmp/nv_jetson_model --volume /var/run/dbus:/var/run/dbus --volume /var/run/avahi-daemon/socket:/var/run/avahi-daemon/socket --volume /var/run/docker.sock:/var/run/docker.sock --volume /ssd/jetson-containers/data:/data -v /etc/localtime:/etc/localtime:ro -v /etc/timezone:/etc/timezone:ro --device /dev/snd -e PULSE_SERVER=unix:/run/user/1000/pulse/native -v /run/user/1000/pulse:/run/user/1000/pulse --device /dev/bus/usb --device /dev/i2c-0 --device /dev/i2c-1 --device /dev/i2c-2 --device /dev/i2c-4 --device /dev/i2c-5 --device /dev/i2c-7 -v /run/jtop.sock:/run/jtop.sock --name mlc-server-${MAKE_CMD} -p 9010:9010 -v /ssd/workspace:/workspace -v /ssd/models:/data/models -v /ssd/models:/root/.cache --entrypoint bash registry.gitlab.com/skails/jetson-nano/venv-3.10-llama_cpp-nanollm-cuda:1.0-arm64 -c "cd /workspace/nanollm-sanic-inference-server && make ${MAKE_CMD}" 